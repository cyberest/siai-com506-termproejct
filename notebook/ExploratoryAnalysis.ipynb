{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# References\n",
        "# ===================\n",
        "\n",
        "# Web scraping tips for Reddit\n",
        "# https://www.datacamp.com/tutorial/scraping-reddit-python-scrapy\n",
        "# https://www.bestproxyreviews.com/reddit-scraper/\n",
        "\n",
        "# Web scraping\n",
        "# https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/\n",
        "\n",
        "# Constructing JSON schema\n",
        "# https://madplay.github.io/post/multiple-and-conditional-json-schemas-validation-examples\n",
        "\n",
        "# Official Reddit API JSON structure\n",
        "# https://github.com/reddit-archive/reddit/wiki/JSON"
      ],
      "metadata": {
        "id": "Jc3CTd_NbXgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library import\n",
        "# ===================\n",
        "\n",
        "import time\n",
        "from timeit import default_timer as timer\n",
        "import random\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "import re"
      ],
      "metadata": {
        "id": "fwR-Qbp4p_c0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "# ===================\n",
        "\n",
        "# For the sake of simplicity, use teh old version of Reddit\n",
        "BASE_URL = \"https://old.reddit.com\"\n",
        "DATA_DIR = \"../data/\"\n",
        "IMAGE_DIR = DATA_DIR + \"images/\" # workdir is /notebook\n",
        "\n",
        "# Handful of User-agents to minimize the risk of connection block\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 12_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/71.0.3578.89 Mobile/15E148 Safari/605.1\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:99.0) Gecko/20100101 Firefox/99.0\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36 Edg/100.0.1185.29\",\n",
        "    \"Mozilla/5.0 (Windows NT 5.1; rv:68.0) Gecko/20100101 Goanna/4.8 Firefox/68.0 PaleMoon/28.10.6a1\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Linux; Android 10; SM-G996U Build/QP1A.190711.020; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Mobile Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Linux; Android 10; Google Pixel 4 Build/QD1A.190821.014.C2; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/78.0.3904.108 Mobile Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone12,1; U; CPU iPhone OS 13_0 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) Version/10.0 Mobile/15E148 Safari/602.1\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\",\n",
        "    \"Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1\",\n",
        "    \"Mozilla/5.0 (CrKey armv7l 1.5.16041) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.0 Safari/537.36\",\n",
        "    \"Roku4640X/DVP-7.70 (297.70E04154A)\",\n",
        "    \"Mozilla/5.0 (Linux; Android 5.1; AFTS Build/LMY47O) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/41.99900.2250.0242 Safari/537.36\",\n",
        "    \"AppleTV11,1/11.1\",\n",
        "    \"Mozilla/5.0 (PlayStation; PlayStation 5/2.26) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; Xbox; Xbox Series X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.82 Safari/537.36 Edge/20.02\",\n",
        "]\n",
        "\n",
        "# List of /r/wallstreetbets moderators, which only provided when user is logged in\n",
        "LIST_MODERATORS = [\n",
        "    \"OPINION_IS_UNPOPULAR\",\n",
        "    \"CHAINSAW_VASECTOMY\",\n",
        "    \"WallStreetBot\",\n",
        "    \"bawse1\",\n",
        "    \"notmikjaash\",\n",
        "    \"Plechazunga_\",\n",
        "    \"HellzAngelz\",\n",
        "    \"Stylux\",\n",
        "    \"ThetaGang_wsb\",\n",
        "    \"Grumpy-james\",\n",
        "]"
      ],
      "metadata": {
        "id": "2gWAns38VCU1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Although Reddit disallows all scraping from all sources (bots)\n",
        "# We have proceeded here for academic purpose\n",
        "print(requests.get(BASE_URL + '/robots.txt').text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrodga_MI3dB",
        "outputId": "4cba0bfe-d43d-4bc6-84a1-66f106624452"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Agent: *\n",
            "Disallow: /\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom functions\n",
        "# ===================\n",
        "\n",
        "# Wait for some random time to avoid IP ban\n",
        "# Official Reddit API allows 60 calls per minute\n",
        "# https://github.com/reddit-archive/reddit/wiki/API#rules\n",
        "def rand_sleep(sec=0):\n",
        "    time.sleep(sec * (random.random()+1))\n",
        "\n",
        "# Store images into filesystem instead of database\n",
        "def save_image(url):\n",
        "    rand_sleep()\n",
        "\n",
        "    # restart Tor to get new proxy\n",
        "    os.system('killall tor > /dev/null')\n",
        "    os.system('service tor start > /dev/null')\n",
        "\n",
        "    session = requests.session()\n",
        "    session.proxies = {\n",
        "        'http': 'socks5h://127.0.0.1:9050',\n",
        "        'https': 'socks5h://127.0.0.1:9050'\n",
        "    }\n",
        "    resp = session.get(url, stream=True)\n",
        "\n",
        "    # Extract filename from full url\n",
        "    file_name = re.search('[^/\\\\&\\?]+\\.\\w{3,4}(?=([\\?&].*$|$))', url).group()\n",
        "    save_path = IMAGE_DIR + file_name\n",
        "    with open(save_path, 'wb') as f:\n",
        "        resp.raw.decode_content=True # force to decompress GZIP or deflate\n",
        "        shutil.copyfileobj(resp.raw, f) # stream data to file object\n",
        "    # Rename file as MD5 hash\n",
        "    new_path = IMAGE_DIR + hash_file(save_path) + '.' + file_name.split('.')[1]\n",
        "    os.rename(save_path, new_path)\n",
        "    return new_path\n",
        "\n",
        "# Hash file to get unique identifier to be stored in DB\n",
        "def hash_file(file_path):\n",
        "    BUFFER_SIZE = 65536  # read stuff in 64kb chunks\n",
        "    md5 = hashlib.md5()\n",
        "\n",
        "    with open(file_path, 'rb') as f:\n",
        "        while True:\n",
        "            data = f.read(BUFFER_SIZE)\n",
        "            if not data:\n",
        "                break\n",
        "            md5.update(data)\n",
        "    return md5.hexdigest()"
      ],
      "metadata": {
        "id": "WUJ5wipnVBq6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "sgTPacuMci0L"
      },
      "outputs": [],
      "source": [
        "class RedditScraper():\n",
        "\n",
        "    def __init__(self, subreddit=\"wallstreetbets\"):\n",
        "        self.base_url = BASE_URL\n",
        "        self.subreddit = subreddit.strip()\n",
        "        self.url = BASE_URL + \"/r/\" + self.subreddit\n",
        "        self.threads = [] # list of dicts\n",
        "        self.threads_df = None # Pandas DataFrame\n",
        "\n",
        "    # Cleanup data upon closing instance\n",
        "    def cleanup(self):\n",
        "        # Remove all images downloaded\n",
        "        shutil.rmtree(IMAGE_DIR)\n",
        "        os.makedirs(IMAGE_DIR)\n",
        "\n",
        "    # Initiate Selenium and Chorme webdriver\n",
        "    def init_selenium_driver(self):\n",
        "        # restart Tor to get new proxy\n",
        "        os.system('killall tor > /dev/null')\n",
        "        os.system('service tor start > /dev/null')\n",
        "\n",
        "        # Brower setup for Selenium\n",
        "        service = Service(executable_path=\"/root/chromedriver\")\n",
        "        options = webdriver.ChromeOptions()\n",
        "        options.add_argument('--headless')\n",
        "        options.add_argument('--no-sandbox')\n",
        "        options.add_argument('--disable-dev-shm-usage')\n",
        "        options.add_argument('user-agent={0}'.format(random.choice(USER_AGENTS)))\n",
        "        # Used proxy to avoid IP ban\n",
        "        options.add_argument('--proxy-server=socks5://127.0.0.1:9050')\n",
        "\n",
        "        driver = webdriver.Chrome(service=service, options=options)\n",
        "        print(\"Selenium initiated.\")\n",
        "        return driver\n",
        "\n",
        "    # Load page source and unfold javascript-powered elements\n",
        "    def load_page_source(self, driver, url):\n",
        "        driver.get(url)\n",
        "        # Unlike time.sleep, continues if webpage is loaded before max time\n",
        "        # driver.implicitly_wait(5)\n",
        "\n",
        "        # Collapsed content area should be expanded to get contents\n",
        "        collapsed_expand_obj = driver.find_elements(\n",
        "            By.XPATH, \n",
        "            \"//div[contains(@class, 'expando-button') and contains(@class, 'collapsed')]\"\n",
        "        );\n",
        "        for expand_button in collapsed_expand_obj:\n",
        "            # click to expand content area\n",
        "            expand_button.click()\n",
        "\n",
        "            # (Explicitly) wait until (javescript-based) content area is fully loaded\n",
        "            post_obj = expand_button.find_element(By.XPATH, \"../..\") # select grandparent\n",
        "            content_obj = post_obj.find_element(By.CLASS_NAME, \"expando\") # where content is shown\n",
        "            WebDriverWait(driver, 20).until(EC.visibility_of(content_obj))\n",
        "\n",
        "        print('BeautifulSoup: page source loaded succesfully.')\n",
        "        return BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Parse post element into data\n",
        "    def parse_post_object(self, post):\n",
        "        # Scrape post information\n",
        "        post_id = post.attrs[\"id\"]\n",
        "        post_link = post.attrs[\"data-permalink\"]\n",
        "        # Datetime submitted in format yyyy-MM-ddThh:mm:ss+00:00 (UTC)\n",
        "        post_time = post.find(\"time\").attrs[\"datetime\"]\n",
        "        post_title = post.find(\"a\", class_=\"title\").text\n",
        "        post_domain = post.find(\"span\", class_=\"domain\").a.text\n",
        "\n",
        "        board_name = post.attrs['data-subreddit']\n",
        "        board_id = post.attrs['data-subreddit-fullname']\n",
        "        board_type = post.attrs['data-subreddit-type']\n",
        "\n",
        "        author_name = post.find(\"a\", class_=\"author\").text\n",
        "        author_id = post.attrs['data-author-fullname']\n",
        "        # Moderator list is hard-coded for /r/wallstreetbets\n",
        "        author_ismod = (author_name in LIST_MODERATORS)\n",
        "        \n",
        "        post_likes = post.find(\"div\", attrs={\"class\": \"score unvoted\"})\n",
        "        post_likes = 0 if post_likes.text == \"•\" else int(post_likes.attrs['title'])\n",
        "        \n",
        "        # Number of comments\n",
        "        post_comments = post.find(\"a\", class_=\"comments\").text.split()[0]\n",
        "        if post_comments == \"comment\":\n",
        "            post_comments = 0\n",
        "        post_comments = int(post_comments)\n",
        "\n",
        "        # Flair attached to the post                \n",
        "        post_flair = post.find(\"span\", class_=\"linkflairlabel\")\n",
        "        if post_flair:\n",
        "            post_flair = post_flair.text\n",
        "\n",
        "        # Awards conveyed to the author\n",
        "        author_awards = {}\n",
        "        awards = post.find(\"span\", class_=\"awardings-bar\").find_all(\"a\", class_=\"awarding-link\")\n",
        "        if awards:\n",
        "            for award in awards:\n",
        "                k = award.attrs['data-award-id']\n",
        "                v = award.attrs['data-count']\n",
        "                author_awards[k] = v\n",
        "\n",
        "        # 4 types of post in terms of its content: \n",
        "        # text only, (single) video, (multiple) images, (single) image\n",
        "        post_content = None\n",
        "        post_media = []\n",
        "        content_area = post.find(\"div\", class_=\"expando\")\n",
        "\n",
        "        if content_area:\n",
        "            # if content loading is not finished, mark it as ERROR\n",
        "            if content_area.find(\"span\", class_=\"error\"):\n",
        "                post_content = \"ERROR\"\n",
        "            else:\n",
        "                content_text = content_area.find(\"div\", class_=\"usertext-body\")\n",
        "                post_content = content_text.text if content_text else None\n",
        "\n",
        "                # Extract media from content\n",
        "                is_video = content_area.find(\"div\", class_=\"video-player\")\n",
        "                is_gallery = content_area.find(\"div\", class_=\"media-gallery\")\n",
        "                is_image = content_area.find(\"div\", class_=\"media-preview-content\")\n",
        "\n",
        "                # Skip if content is video\n",
        "                if is_video:\n",
        "                    pass\n",
        "                # if content is multiple images\n",
        "                elif is_gallery:\n",
        "                    imgs = content_area.find_all(\"div\", class_=\"media-preview-content\")\n",
        "                    for img in imgs:\n",
        "                        # do not store image(file) into database\n",
        "                        img_path = save_image(img.find(\"img\")[\"src\"])\n",
        "                        # instead, store file hash as a link to saved file\n",
        "                        img_hash = hash_file(img_path)\n",
        "                        post_media.append(img_hash)\n",
        "                # if content is single image\n",
        "                elif is_image:\n",
        "                    img_path = save_image(is_image.find(\"img\")[\"src\"])\n",
        "                    img_hash = hash_file(img_path)\n",
        "                    post_media.append(img_hash)\n",
        "\n",
        "        self.threads.append({\n",
        "            \"post_id\": post_id, \n",
        "            \"link\": post_link,\n",
        "            \"time\": post_time, \n",
        "            \"title\": post_title, \n",
        "            \"domain\": post_domain,\n",
        "            \"board_name\": board_name,\n",
        "            \"board_id\": board_id,\n",
        "            \"board_type\": board_type,\n",
        "            \"author_name\": author_name, \n",
        "            \"author_id\": author_id, \n",
        "            \"author_ismod\": author_ismod,\n",
        "            \"likes\": post_likes, \n",
        "            \"comments\": post_comments, \n",
        "            \"flair\": post_flair,\n",
        "            \"author_awards\": author_awards,\n",
        "            \"content\": post_content,\n",
        "            \"media\": post_media,\n",
        "        })\n",
        "\n",
        "    # Start scraping up to designated number of items\n",
        "    def scrape_threads(self, max_item=0):\n",
        "        # Measures time spent\n",
        "        start_time = timer()\n",
        "        # Disguise headers to avoid IP ban\n",
        "        headers = {\n",
        "            \"User-Agent\": random.choice(USER_AGENTS),\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"    \n",
        "        }\n",
        "        driver = self.init_selenium_driver()\n",
        "        bs = self.load_page_source(driver, self.url)\n",
        "        driver.quit() # Close browser\n",
        "\n",
        "        # Get all threads regardless of domain direction (inbound / outbound)\n",
        "        attrs = {\"class\": \"thing\", \"id\": True}\n",
        "        page_counter = 0\n",
        "        post_counter = 0\n",
        "\n",
        "        # Loop until next page does not exists\n",
        "        while True:\n",
        "            page_counter += 1\n",
        "            for post in bs.find_all(\"div\", attrs=attrs):\n",
        "\n",
        "                # Skip if the post is already collected\n",
        "                if post.attrs[\"id\"] in [p[\"post_id\"] for p in self.threads]:\n",
        "                    continue\n",
        "\n",
        "                # Skip advertisement posts\n",
        "                if post.find(\"span\", class_=\"promoted-tag\"):\n",
        "                    continue\n",
        "\n",
        "                # Show progress for debug\n",
        "                post_counter += 1\n",
        "                if post_counter % 5 == 1:\n",
        "                    print(\"Processing post #{} from page #{}...\".format(post_counter, page_counter))\n",
        "\n",
        "                # Parse and store post object\n",
        "                self.parse_post_object(post)\n",
        "\n",
        "                # Break if reached target item count\n",
        "                if (max_item > 0) and (post_counter >= max_item):\n",
        "                    break\n",
        "\n",
        "            # For every page request, pause not to reach rate limit\n",
        "            rand_sleep()\n",
        "\n",
        "            # Skipped pages afterwards due to IP ban\n",
        "            if page_counter == 1:\n",
        "                break\n",
        "\n",
        "            next_button = bs.find(\"span\", class_=\"next-button\")\n",
        "            # continue scraping only when next page exists\n",
        "            if next_button:\n",
        "                next_page_link = next_button.find(\"a\").attrs['href']\n",
        "                bs = self.load_page_source(driver, next_page_link)\n",
        "                driver.quit() # Close browser\n",
        "            # next button no longer exists after 500 newest posts\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        print(f'Scraping completed - collected (additional) {post_counter} post(s) from {page_counter} page(s) in {timer()-start_time:,.1f} seconds.')\n",
        "        print(f'Returned {len(self.threads)} records.')\n",
        "        return self.threads"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate scraper\n",
        "scraper = RedditScraper(subreddit=\"wallstreetbets\")"
      ],
      "metadata": {
        "id": "9JAmor0YWU0i"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect posts\n",
        "result = scraper.scrape_threads()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_ZwTxEJ29aS",
        "outputId": "27ae7623-705a-4765-a7a6-6dc7759696df"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selenium initiated.\n",
            "BeautifulSoup: page source loaded succesfully.\n",
            "Processing post #1 from page #1...\n",
            "Processing post #6 from page #1...\n",
            "Processing post #11 from page #1...\n",
            "Processing post #16 from page #1...\n",
            "Processing post #21 from page #1...\n",
            "Processing post #26 from page #1...\n",
            "Scraping completed - collected (additional) 27 post(s) from 1 page(s) in 187.0 seconds.\n",
            "Returned 27 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Redundant order does not create duplicates\n",
        "result = scraper.scrape_threads()"
      ],
      "metadata": {
        "id": "8Jk5CoFTCqe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4787e57e-6766-485b-ac25-7298a6e4a6aa"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selenium initiated.\n",
            "BeautifulSoup: page source loaded succesfully.\n",
            "Scraping completed - collected (additional) 0 post(s) from 1 page(s) in 38.9 seconds.\n",
            "Returned 27 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Library import\n",
        "# ===================\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "import sqlite3\n",
        "from elasticsearch import Elasticsearch, helpers"
      ],
      "metadata": {
        "id": "WG9ucULNekrr"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"../output/\"\n",
        "ELASTIC_SERVER = \"http://es01:9200\"\n",
        "NOSQL_MAPPING = \"../config/NoSQLmap.json\""
      ],
      "metadata": {
        "id": "8o95OUErlO_Z"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Inherited RedditScraper and added Database operations\"\"\"\n",
        "class RedditDataManager(RedditScraper):\n",
        "\n",
        "    # Reset database to initialize project\n",
        "    def reset_db(self):\n",
        "        pass\n",
        "\n",
        "    # Download collected records in CSV format\n",
        "    def download_csv(self):\n",
        "\n",
        "        columns_headers = list(self.threads[0].keys())\n",
        "        file_name = f'{self.subreddit}_{int(time.time())}.csv'\n",
        "\n",
        "        with open(OUTPUT_DIR + file_name, 'a', encoding='utf-8') as f:\n",
        "            # used tab delimiter - content contains many commas\n",
        "            writer = csv.DictWriter(f, quoting=csv.QUOTE_ALL, fieldnames=columns_headers)\n",
        "            writer.writerow(dict((fn,fn) for fn in columns_headers))\n",
        "            for thread in self.threads:\n",
        "                writer.writerow(thread)\n",
        "        print(f'CSV export completeted: {file_name}')\n",
        "\n",
        "    # Converts collected records to Pandas DataFrame\n",
        "    def threads_to_df(self):\n",
        "\n",
        "        if self.threads:\n",
        "            df = pd.DataFrame(self.threads)\n",
        "\n",
        "            # Datatype adjustments for DB upload\n",
        "            # df['time'] = pd.to_datetime(df['time']) # object to datetime64, not compatible for sqlite\n",
        "            df.loc[df['author_awards'].apply(len).eq(0), 'author_awards'] = None # replace {} with None\n",
        "            df['author_awards'] = df['author_awards'].apply(json.dumps) # Serialize dict with JSON\n",
        "            df.loc[df['media'].apply(len).eq(0), 'media'] = None # replace [] with None\n",
        "            df['media'] = df['media'].apply(json.dumps) # Serialize list with JSON\n",
        "\n",
        "            self.threads_df = df\n",
        "            return df\n",
        "    \n",
        "    # Establish connection to NoSQL server\n",
        "    def sql_connection(self):\n",
        "        conn = None\n",
        "        try:\n",
        "            conn = sqlite3.connect(DATA_DIR + f\"sqlite/reddit_scrap.db\")\n",
        "        except Excetpion as e:\n",
        "            print(\"Failed to establish SQL connection: \", e)\n",
        "        return conn\n",
        "\n",
        "    # Upload collected records into SQL database\n",
        "    def upload_sql(self):\n",
        "        conn = self.sql_connection()\n",
        "        try:\n",
        "            cur = conn.cursor()\n",
        "            conn.execute(f\"\"\"CREATE TABLE IF NOT EXISTS reddit_thread_{self.subreddit}(\n",
        "                post_id TEXT PRIMARY KEY,\n",
        "                link TEXT NOT NULL,\n",
        "                time TEXT NOT NULL,\n",
        "                title TEXT NOT NULL,\n",
        "                domain TEXT,\n",
        "                board_name TEXT,\n",
        "                board_id TEXT,\n",
        "                board_type TEXT,\n",
        "                author_name TEXT NOT NULL,\n",
        "                author_id TEXT NOT NULL,\n",
        "                author_ismod INTEGER NOT NULL,\n",
        "                likes INTEGER NOT NULL,\n",
        "                comments INTEGER NOT NULL,\n",
        "                flair TEXT,\n",
        "                author_awards TEXT,\n",
        "                content TEXT,\n",
        "                media TEXT\n",
        "            )\"\"\")\n",
        "            # list of tuples needed as cursor.executemary() input\n",
        "            list_of_tuple = list(self.threads_df.itertuples(index=False, name=None))\n",
        "            cur.executemany( # Use INSERT IGNORE not to have primary key confliction\n",
        "                f\"\"\"INSERT OR IGNORE INTO reddit_thread_{self.subreddit} \n",
        "                VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\",\n",
        "                list_of_tuple\n",
        "            )\n",
        "            conn.commit()\n",
        "            print(f\"SQL upload succeeded: {len(list_of_tuple)} records processed.\")\n",
        "        except Exception as e:\n",
        "            print(\"SQL upload failed:\", e)\n",
        "        finally:\n",
        "            conn.close() # prevents database locked error\n",
        "\n",
        "    # Execute query on SQL database\n",
        "    def query_sql(self, sql):\n",
        "        conn = self.sql_connection()\n",
        "        try:\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(sql)\n",
        "            \n",
        "            records = cur.fetchall()\n",
        "            columns_headers = [cols[0] for cols in cur.description]\n",
        "\n",
        "            print(f\"SQL query succeeded: {len(records)} records fetched.\")\n",
        "            return pd.DataFrame.from_records(data=records, columns=columns_headers)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"SQL query failed:\", e)\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "    def drop_sql_table(self, table):\n",
        "        conn = self.sql_connection()\n",
        "        try:\n",
        "            cur = conn.cursor()\n",
        "            cur.execute(f\"DROP table {table};\")\n",
        "            print(f\"SQL table dropped: {table}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"SQL table drop failed:\", e)\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "    # Establish connection to NoSQL server\n",
        "    def nosql_connection(self):\n",
        "        try:\n",
        "            es = Elasticsearch(ELASTIC_SERVER)\n",
        "        except Exception as e:\n",
        "            print(\"Failed to establish NoSQL connection: \", e)\n",
        "        return es        \n",
        "\n",
        "    # Upload collected records into NoSQL database\n",
        "    def upload_nosql(self):\n",
        "        es = self.nosql_connection()\n",
        "        es_index_name = f\"reddit_thread_{self.subreddit}\"\n",
        "\n",
        "        # Loads json schema\n",
        "        with open(NOSQL_MAPPING, 'r') as j:\n",
        "            json_schema = json.loads(j.read())\n",
        "\n",
        "        # Create index with predefined mappings if not exist\n",
        "        if es.indices.exists(index=es_index_name):\n",
        "            pass\n",
        "        else:\n",
        "            res = es.indices.create(index=es_index_name, **json_schema)\n",
        "            print(res)\n",
        "\n",
        "        # Insert data using Elasticsearch bulk API\n",
        "        list_of_dicts = manager.threads_df.to_dict('records')\n",
        "        bulk_doc = [\n",
        "            {\n",
        "                \"_index\": es_index_name,\n",
        "                \"_source\": a_record\n",
        "            } for a_record in list_of_dicts\n",
        "        ]\n",
        "        res = helpers.bulk(es, bulk_doc)\n",
        "        print(res)\n",
        "        es.close()\n",
        "\n",
        "    # Execute query on NoSQL database\n",
        "    def query_nosql(self, query):\n",
        "        es = self.nosql_connection()\n",
        "        es_index_name = f\"reddit_thread_{self.subreddit}\"\n",
        "\n",
        "        res = es.search(index=es_index_name, query=query)\n",
        "        print(f\"Total {res['hits']['total']['value']} records returned.\")\n",
        "        es.close()\n",
        "        return res['hits']['hits']\n",
        "\n",
        "    def drop_nosql_index(self, index):\n",
        "        es = self.nosql_connection()\n",
        "\n",
        "        res = es.indices.delete(\n",
        "            index=index, \n",
        "            ignore=[400, 404] # ignores error when index does not exist\n",
        "        )\n",
        "        print(res)\n"
      ],
      "metadata": {
        "id": "tAKd9ebyWdml"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate data manager\n",
        "manager = RedditDataManager(subreddit=\"wallstreetbets\")\n",
        "# Copy collected data into inherited class\n",
        "manager.threads = scraper.threads\n",
        "\n",
        "# Ready to upload to database\n",
        "data_df = manager.threads_to_df()\n",
        "data_df.head(3)"
      ],
      "metadata": {
        "id": "N7Qt1dq9U4rB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "be575359-d22e-4f83-c071-4b2a11b669b8"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           post_id                                               link  \\\n",
              "0  thing_t3_xa4z97  /r/wallstreetbets/comments/xa4z97/weekend_disc...   \n",
              "1  thing_t3_x4ryjg  /r/wallstreetbets/comments/x4ryjg/most_anticip...   \n",
              "2  thing_t3_xadm21  /r/wallstreetbets/comments/xadm21/anyone_hirin...   \n",
              "\n",
              "                        time  \\\n",
              "0  2022-09-09T20:00:11+00:00   \n",
              "1  2022-09-03T11:16:01+00:00   \n",
              "2  2022-09-10T02:27:59+00:00   \n",
              "\n",
              "                                               title               domain  \\\n",
              "0  Weekend Discussion Thread for the Weekend of S...  self.wallstreetbets   \n",
              "1  Most Anticipated Earnings Releases for the wee...            i.redd.it   \n",
              "2  Anyone hiring ? So over this. I have a degree ...            i.redd.it   \n",
              "\n",
              "       board_name  board_id board_type           author_name    author_id  \\\n",
              "0  wallstreetbets  t5_2th52     public  OPINION_IS_UNPOPULAR     t2_bd6q5   \n",
              "1  wallstreetbets  t5_2th52     public           bigbear0083     t2_eaak0   \n",
              "2  wallstreetbets  t5_2th52     public   Jellyfish_Vegetable  t2_7x5hixx2   \n",
              "\n",
              "   author_ismod  likes  comments               flair  \\\n",
              "0          True    103      4789  Weekend Discussion   \n",
              "1         False   1724       812     Earnings Thread   \n",
              "2         False   1426       519                Loss   \n",
              "\n",
              "                                       author_awards  \\\n",
              "0  {\"award_f44611f1-b89e-46dc-97fe-892280b13b82\":...   \n",
              "1  {\"gid_1\": \"4\", \"award_f44611f1-b89e-46dc-97fe-...   \n",
              "2  {\"award_abcdefe4-c92f-4c66-880f-425962d17098\":...   \n",
              "\n",
              "                                             content  \\\n",
              "0  Read rules, follow Twitter and IG, join Discor...   \n",
              "1                                               None   \n",
              "2                                               None   \n",
              "\n",
              "                                  media  \n",
              "0                                  null  \n",
              "1  [\"e823d70122c40c36bc6526b7527f1d54\"]  \n",
              "2  [\"48bc5978a8c2e71ff7e0a1399586dd81\"]  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>link</th>\n",
              "      <th>time</th>\n",
              "      <th>title</th>\n",
              "      <th>domain</th>\n",
              "      <th>board_name</th>\n",
              "      <th>board_id</th>\n",
              "      <th>board_type</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_id</th>\n",
              "      <th>author_ismod</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>flair</th>\n",
              "      <th>author_awards</th>\n",
              "      <th>content</th>\n",
              "      <th>media</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thing_t3_xa4z97</td>\n",
              "      <td>/r/wallstreetbets/comments/xa4z97/weekend_disc...</td>\n",
              "      <td>2022-09-09T20:00:11+00:00</td>\n",
              "      <td>Weekend Discussion Thread for the Weekend of S...</td>\n",
              "      <td>self.wallstreetbets</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>OPINION_IS_UNPOPULAR</td>\n",
              "      <td>t2_bd6q5</td>\n",
              "      <td>True</td>\n",
              "      <td>103</td>\n",
              "      <td>4789</td>\n",
              "      <td>Weekend Discussion</td>\n",
              "      <td>{\"award_f44611f1-b89e-46dc-97fe-892280b13b82\":...</td>\n",
              "      <td>Read rules, follow Twitter and IG, join Discor...</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thing_t3_x4ryjg</td>\n",
              "      <td>/r/wallstreetbets/comments/x4ryjg/most_anticip...</td>\n",
              "      <td>2022-09-03T11:16:01+00:00</td>\n",
              "      <td>Most Anticipated Earnings Releases for the wee...</td>\n",
              "      <td>i.redd.it</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>bigbear0083</td>\n",
              "      <td>t2_eaak0</td>\n",
              "      <td>False</td>\n",
              "      <td>1724</td>\n",
              "      <td>812</td>\n",
              "      <td>Earnings Thread</td>\n",
              "      <td>{\"gid_1\": \"4\", \"award_f44611f1-b89e-46dc-97fe-...</td>\n",
              "      <td>None</td>\n",
              "      <td>[\"e823d70122c40c36bc6526b7527f1d54\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thing_t3_xadm21</td>\n",
              "      <td>/r/wallstreetbets/comments/xadm21/anyone_hirin...</td>\n",
              "      <td>2022-09-10T02:27:59+00:00</td>\n",
              "      <td>Anyone hiring ? So over this. I have a degree ...</td>\n",
              "      <td>i.redd.it</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>Jellyfish_Vegetable</td>\n",
              "      <td>t2_7x5hixx2</td>\n",
              "      <td>False</td>\n",
              "      <td>1426</td>\n",
              "      <td>519</td>\n",
              "      <td>Loss</td>\n",
              "      <td>{\"award_abcdefe4-c92f-4c66-880f-425962d17098\":...</td>\n",
              "      <td>None</td>\n",
              "      <td>[\"48bc5978a8c2e71ff7e0a1399586dd81\"]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export as CSV\n",
        "manager.download_csv()"
      ],
      "metadata": {
        "id": "R1uBUz7xWQuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b410818d-e6cc-4f43-8723-4ccf857ce3d2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV export completeted: wallstreetbets_1662789771.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload DataFrame to SQL database\n",
        "manager.upload_sql()"
      ],
      "metadata": {
        "id": "LPmiogiYBMKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a25a05-f24b-4174-f098-c18a4a1662cb"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL upload succeeded: 27 records processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_table = f\"reddit_thread_{manager.subreddit}\"\n",
        "query_sql = f\"SELECT * FROM {target_table}\"\n",
        "\n",
        "query_data = manager.query_sql(query_sql)\n",
        "query_data.head()"
      ],
      "metadata": {
        "id": "t401OSIlWSkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "108075b2-9a62-4314-e08b-27322e2d9516"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL query succeeded: 27 records fetched.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           post_id                                               link  \\\n",
              "0  thing_t3_xa4z97  /r/wallstreetbets/comments/xa4z97/weekend_disc...   \n",
              "1  thing_t3_x4ryjg  /r/wallstreetbets/comments/x4ryjg/most_anticip...   \n",
              "2  thing_t3_xadm21  /r/wallstreetbets/comments/xadm21/anyone_hirin...   \n",
              "3  thing_t3_x2xl6r  /r/wallstreetbets/comments/x2xl6r/wallstreetbe...   \n",
              "4  thing_t3_x9ye6u  /r/wallstreetbets/comments/x9ye6u/bbby_got_me_...   \n",
              "\n",
              "                        time  \\\n",
              "0  2022-09-09T20:00:11+00:00   \n",
              "1  2022-09-03T11:16:01+00:00   \n",
              "2  2022-09-10T02:27:59+00:00   \n",
              "3  2022-09-09T11:27:26+00:00   \n",
              "4  2022-09-09T15:26:44+00:00   \n",
              "\n",
              "                                               title               domain  \\\n",
              "0  Weekend Discussion Thread for the Weekend of S...  self.wallstreetbets   \n",
              "1  Most Anticipated Earnings Releases for the wee...            i.redd.it   \n",
              "2  Anyone hiring ? So over this. I have a degree ...            i.redd.it   \n",
              "3  🔮WallStreetBets Predictions Tournament for Sep...           reddit.com   \n",
              "4                                  $BBBY got me like            i.redd.it   \n",
              "\n",
              "       board_name  board_id board_type           author_name    author_id  \\\n",
              "0  wallstreetbets  t5_2th52     public  OPINION_IS_UNPOPULAR     t2_bd6q5   \n",
              "1  wallstreetbets  t5_2th52     public           bigbear0083     t2_eaak0   \n",
              "2  wallstreetbets  t5_2th52     public   Jellyfish_Vegetable  t2_7x5hixx2   \n",
              "3  wallstreetbets  t5_2th52     public         ThetaGang_wsb  t2_amboe4pe   \n",
              "4  wallstreetbets  t5_2th52     public           Kieran30803   t2_12t1meg   \n",
              "\n",
              "   author_ismod  likes  comments               flair  \\\n",
              "0             1    103      4789  Weekend Discussion   \n",
              "1             0   1724       812     Earnings Thread   \n",
              "2             0   1426       519                Loss   \n",
              "3             1  27281       131                None   \n",
              "4             0   9391       171                Meme   \n",
              "\n",
              "                                       author_awards  \\\n",
              "0  {\"award_f44611f1-b89e-46dc-97fe-892280b13b82\":...   \n",
              "1  {\"gid_1\": \"4\", \"award_f44611f1-b89e-46dc-97fe-...   \n",
              "2  {\"award_abcdefe4-c92f-4c66-880f-425962d17098\":...   \n",
              "3  {\"gid_1\": \"1\", \"award_f44611f1-b89e-46dc-97fe-...   \n",
              "4  {\"gid_3\": \"1\", \"award_f44611f1-b89e-46dc-97fe-...   \n",
              "\n",
              "                                             content  \\\n",
              "0  Read rules, follow Twitter and IG, join Discor...   \n",
              "1                                               None   \n",
              "2                                               None   \n",
              "3                                               None   \n",
              "4                                               None   \n",
              "\n",
              "                                  media  \n",
              "0                                  null  \n",
              "1  [\"e823d70122c40c36bc6526b7527f1d54\"]  \n",
              "2  [\"48bc5978a8c2e71ff7e0a1399586dd81\"]  \n",
              "3                                  null  \n",
              "4  [\"923230d128904d88c16dae27522a7412\"]  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>link</th>\n",
              "      <th>time</th>\n",
              "      <th>title</th>\n",
              "      <th>domain</th>\n",
              "      <th>board_name</th>\n",
              "      <th>board_id</th>\n",
              "      <th>board_type</th>\n",
              "      <th>author_name</th>\n",
              "      <th>author_id</th>\n",
              "      <th>author_ismod</th>\n",
              "      <th>likes</th>\n",
              "      <th>comments</th>\n",
              "      <th>flair</th>\n",
              "      <th>author_awards</th>\n",
              "      <th>content</th>\n",
              "      <th>media</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thing_t3_xa4z97</td>\n",
              "      <td>/r/wallstreetbets/comments/xa4z97/weekend_disc...</td>\n",
              "      <td>2022-09-09T20:00:11+00:00</td>\n",
              "      <td>Weekend Discussion Thread for the Weekend of S...</td>\n",
              "      <td>self.wallstreetbets</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>OPINION_IS_UNPOPULAR</td>\n",
              "      <td>t2_bd6q5</td>\n",
              "      <td>1</td>\n",
              "      <td>103</td>\n",
              "      <td>4789</td>\n",
              "      <td>Weekend Discussion</td>\n",
              "      <td>{\"award_f44611f1-b89e-46dc-97fe-892280b13b82\":...</td>\n",
              "      <td>Read rules, follow Twitter and IG, join Discor...</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thing_t3_x4ryjg</td>\n",
              "      <td>/r/wallstreetbets/comments/x4ryjg/most_anticip...</td>\n",
              "      <td>2022-09-03T11:16:01+00:00</td>\n",
              "      <td>Most Anticipated Earnings Releases for the wee...</td>\n",
              "      <td>i.redd.it</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>bigbear0083</td>\n",
              "      <td>t2_eaak0</td>\n",
              "      <td>0</td>\n",
              "      <td>1724</td>\n",
              "      <td>812</td>\n",
              "      <td>Earnings Thread</td>\n",
              "      <td>{\"gid_1\": \"4\", \"award_f44611f1-b89e-46dc-97fe-...</td>\n",
              "      <td>None</td>\n",
              "      <td>[\"e823d70122c40c36bc6526b7527f1d54\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thing_t3_xadm21</td>\n",
              "      <td>/r/wallstreetbets/comments/xadm21/anyone_hirin...</td>\n",
              "      <td>2022-09-10T02:27:59+00:00</td>\n",
              "      <td>Anyone hiring ? So over this. I have a degree ...</td>\n",
              "      <td>i.redd.it</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>Jellyfish_Vegetable</td>\n",
              "      <td>t2_7x5hixx2</td>\n",
              "      <td>0</td>\n",
              "      <td>1426</td>\n",
              "      <td>519</td>\n",
              "      <td>Loss</td>\n",
              "      <td>{\"award_abcdefe4-c92f-4c66-880f-425962d17098\":...</td>\n",
              "      <td>None</td>\n",
              "      <td>[\"48bc5978a8c2e71ff7e0a1399586dd81\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thing_t3_x2xl6r</td>\n",
              "      <td>/r/wallstreetbets/comments/x2xl6r/wallstreetbe...</td>\n",
              "      <td>2022-09-09T11:27:26+00:00</td>\n",
              "      <td>🔮WallStreetBets Predictions Tournament for Sep...</td>\n",
              "      <td>reddit.com</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>ThetaGang_wsb</td>\n",
              "      <td>t2_amboe4pe</td>\n",
              "      <td>1</td>\n",
              "      <td>27281</td>\n",
              "      <td>131</td>\n",
              "      <td>None</td>\n",
              "      <td>{\"gid_1\": \"1\", \"award_f44611f1-b89e-46dc-97fe-...</td>\n",
              "      <td>None</td>\n",
              "      <td>null</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thing_t3_x9ye6u</td>\n",
              "      <td>/r/wallstreetbets/comments/x9ye6u/bbby_got_me_...</td>\n",
              "      <td>2022-09-09T15:26:44+00:00</td>\n",
              "      <td>$BBBY got me like</td>\n",
              "      <td>i.redd.it</td>\n",
              "      <td>wallstreetbets</td>\n",
              "      <td>t5_2th52</td>\n",
              "      <td>public</td>\n",
              "      <td>Kieran30803</td>\n",
              "      <td>t2_12t1meg</td>\n",
              "      <td>0</td>\n",
              "      <td>9391</td>\n",
              "      <td>171</td>\n",
              "      <td>Meme</td>\n",
              "      <td>{\"gid_3\": \"1\", \"award_f44611f1-b89e-46dc-97fe-...</td>\n",
              "      <td>None</td>\n",
              "      <td>[\"923230d128904d88c16dae27522a7412\"]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manager.upload_nosql()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pgqZMYGyiYi",
        "outputId": "5ff85050-ffa6-4e1b-93bb-1207579505bf"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'reddit_thread_wallstreetbets'}\n",
            "(27, [])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_nosql = {\n",
        "    \"match\": {\n",
        "        \"board_name\": \"wallstreetbets\"\n",
        "    }\n",
        "}\n",
        "query_data = manager.query_nosql(query_nosql)\n",
        "query_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnG6S_KAy0xC",
        "outputId": "5d6b9984-1a5f-4504-d8d3-08d11e8ed0dd"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 27 records returned.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_index': 'reddit_thread_wallstreetbets',\n",
              " '_type': '_doc',\n",
              " '_id': 'o-UQJoMBacOpM5JcYy6n',\n",
              " '_score': 0.018018505,\n",
              " '_source': {'post_id': 'thing_t3_xa4z97',\n",
              "  'link': '/r/wallstreetbets/comments/xa4z97/weekend_discussion_thread_for_the_weekend_of/',\n",
              "  'time': '2022-09-09T20:00:11+00:00',\n",
              "  'title': 'Weekend Discussion Thread for the Weekend of September 10, 2022',\n",
              "  'domain': 'self.wallstreetbets',\n",
              "  'board_name': 'wallstreetbets',\n",
              "  'board_id': 't5_2th52',\n",
              "  'board_type': 'public',\n",
              "  'author_name': 'OPINION_IS_UNPOPULAR',\n",
              "  'author_id': 't2_bd6q5',\n",
              "  'author_ismod': True,\n",
              "  'likes': 103,\n",
              "  'comments': 4789,\n",
              "  'flair': 'Weekend Discussion',\n",
              "  'author_awards': '{\"award_f44611f1-b89e-46dc-97fe-892280b13b82\": \"2\"}',\n",
              "  'content': 'Read rules, follow Twitter and IG, join Discord, see ban bets!\\nEarnings Thread\\n\\n',\n",
              "  'media': 'null'}}"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "manager.drop_nosql_index(target_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBuqfobE7WhG",
        "outputId": "f94b3c64-8a3b-4a25-edee-e48d79db8f0d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'acknowledged': True}\n"
          ]
        }
      ]
    }
  ]
}